{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-18 15:30:34.268 INFO    rdkit: Enabling RDKit 2020.09.1 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "import scipy.optimize as opt\n",
    "import statistics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import re\n",
    "import matplotlib.backends.backend_pdf\n",
    "import os\n",
    "import time\n",
    "from rdkit import Chem\n",
    "from mordred import Calculator, descriptors\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Ridge\n",
    "from matplotlib import gridspec\n",
    "from PIL import Image\n",
    "from random import Random\n",
    "import scipy.stats as stt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(dataset):\n",
    "    dataset=dataset.sample(frac=1)\n",
    "    length= len(dataset)\n",
    "    eightypercent= int(length*0.8)\n",
    "    trainingset= dataset[:eightypercent]\n",
    "    testset=dataset[eightypercent:]\n",
    "    return trainingset, testset\n",
    "\n",
    "def re_round(li, _prec=3):\n",
    "    ''' Rounds argument li to desired number of decimals (_prec) '''\n",
    "    try:\n",
    "         return round(li, _prec)\n",
    "    except TypeError:\n",
    "         return type(li)(re_round(x, _prec) for x in li)\n",
    "             \n",
    "def findstr(test_str,test_sub):\n",
    "    res = [i for i in range(len(test_str)) if test_str.startswith(test_sub, i)]\n",
    "    return res\n",
    "    \n",
    "def find(s, ch):\n",
    "    return [i for i, ltr in enumerate(s) if ltr == ch]\n",
    "\n",
    "def split_data(dataset):\n",
    "    dataset=dataset.sample(frac=1)\n",
    "    length= len(dataset)\n",
    "    eightypercent= int(length*0.8)\n",
    "    trainingset= dataset[:eightypercent]\n",
    "    testset=dataset[eightypercent:]\n",
    "    return trainingset, testset\n",
    "\n",
    "def fit_tm_model(predictors,*parameter):\n",
    "    parameters= parameter\n",
    "    return eval(model_form)\n",
    "\n",
    "def fit_tm_model_err(predictors,*parameter):\n",
    "    parameters= parameter[0]\n",
    "    return eval(model_form)\n",
    "\n",
    "def make_plots(dataset_test,dataset_train,letters_in_use, dataset_name):\n",
    "    Tbuffer= 25\n",
    "    lowerT =min ( min(dataset_test['T_m (K)']),min(dataset_train['T_m (K)']),min(fit_tm_model_err(dataset_test,letters_in_use)),min (fit_tm_model_err(dataset_train,letters_in_use))) - 273\n",
    "    lowerT=lowerT- Tbuffer\n",
    "\n",
    "    higherT =max( max(dataset_test['T_m (K)']),max(dataset_train['T_m (K)'])\n",
    "    ,max(fit_tm_model_err(dataset_test,\n",
    "    letters_in_use)),max(fit_tm_model_err(dataset_train,letters_in_use))) - 273\n",
    "    higherT=higherT+Tbuffer\n",
    "\n",
    "    \n",
    "    fig, ax=plt.subplots(figsize=(4,4), dpi=300)\n",
    "    \n",
    "    #ax.figure(figsize=(4,4), dpi=300)\n",
    "    ax.plot(dataset_train['T_m (K)']-273,fit_tm_model_err(dataset_train,\n",
    "    letters_in_use)-273,'ko',markersize=4)\n",
    "    ax.plot(dataset_test['T_m (K)']-273,fit_tm_model_err(dataset_test,\n",
    "    letters_in_use)-273,'ro',markersize=4)\n",
    "    ax.plot([lowerT,higherT],[lowerT,higherT],color=((0.6,0.6,0.6)))\n",
    "    ax.set_ylabel('Predicted $T_m$ (°C)')\n",
    "    ax.set_xlabel('Experimental $T_m$ (°C)')\n",
    "    #ax.set_title('Calculated vs Experimental $T_m$ for '+ model_form_name + ' - ' + dataset_name +'\\n Test Error: ' + str(avg_model_err[0])+ '\\n Train Error: ' + str(avg_model_err[1]) + '\\n RMSE Test:'+ str(rmse_err[0])+ '\\n RMSE Train:'+str(rmse_err[1]))\n",
    "    ax.legend(('Training Set','Test Set'))\n",
    "    ax.set_xlim([lowerT,higherT])\n",
    "    ax.set_ylim([lowerT,higherT])\n",
    "    return fig\n",
    "\n",
    "# function to calculate root mean square error\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "# Function for rounding tuples in confidence intervals - copied from online\n",
    "def re_round(li, _prec=3):\n",
    "    ''' Rounds argument li to desired number of decimals (_prec) '''\n",
    "    try:\n",
    "         return round(li, _prec)\n",
    "    except TypeError:\n",
    "         return type(li)(re_round(x, _prec) for x in li)\n",
    "\n",
    "def vbt_model_automated(dataset_dictionary, dataset_name, model_form, starting_guesses, number_of_runs):\n",
    "    '''Generates a dictionary of VBT model plot, Avg Absolute Error, RMSE Error, and Parameter values'''\n",
    "    all_possible_predictors= ['tau', 'V_m (nm3)', 'sigma','Eccentricity(Eal)', 'Eccentricity(Ear)']\n",
    "    letters_in_use= ['a','b','c','d','f','g','h','k','l','m']\n",
    "    predicted= ['T_m (K)']\n",
    "    used_predictors= []\n",
    "    # This for loop finds which parameters are used in the model form and adds them to a list of used predictors\n",
    "    for i in range(len(all_possible_predictors)):\n",
    "        this_predictor= all_possible_predictors[i]\n",
    "        if findstr(model_form, this_predictor):\n",
    "            used_predictors.append(this_predictor)\n",
    "    used_predictors= used_predictors+predicted\n",
    "    num_parameters= len(findstr(model_form, 'parameter'))\n",
    "\n",
    "    if num_parameters!=len(starting_guesses):\n",
    "        raise Exception(\"Number of starting guesses doesn't match the number of parameters\")\n",
    "\n",
    "    letters_in_use=letters_in_use[0:num_parameters]\n",
    "    num_parameters= list(range(num_parameters))\n",
    "    train_avg_err=[]\n",
    "    test_avg_err=[]\n",
    "    train_rmse_err=[]\n",
    "    test_rmse_err=[]\n",
    "    count=0\n",
    "    parameters_from_runs=np.zeros((number_of_runs,len(num_parameters)))\n",
    "    dataset= dataset_dictionary[dataset_name]\n",
    "    #dataset_length= len(dataset)\n",
    "    predictors=dataset[used_predictors].astype('float64')\n",
    "    while count < number_of_runs:\n",
    "        [dataset_train,dataset_test]=split_data(predictors)\n",
    "        (letters_in_use,_)=opt.curve_fit(fit_tm_model,dataset_train,dataset_train['T_m (K)'],(starting_guesses))\n",
    "        parameters_from_runs[count,:]=letters_in_use\n",
    "        train_avg_err.append(statistics.mean(np.absolute(fit_tm_model_err(dataset_train,letters_in_use)-dataset_train['T_m (K)'])))\n",
    "        test_avg_err.append(statistics.mean(np.absolute(fit_tm_model_err(dataset_test,letters_in_use)-dataset_test['T_m (K)'])))\n",
    "        train_rmse_err.append(rmse((fit_tm_model_err(dataset_train,letters_in_use)),dataset_train['T_m (K)']))\n",
    "        test_rmse_err.append(rmse((fit_tm_model_err(dataset_test,letters_in_use)),dataset_test['T_m (K)']))\n",
    "        count = count+1\n",
    "\n",
    "    fig=make_plots(dataset_test,dataset_train,letters_in_use,dataset_name)\n",
    "    \n",
    "    ci_int_val=0.95\n",
    "    dec_points=3\n",
    "\n",
    "    ind_names=[]\n",
    "    for i in range(1,num_runs+1):\n",
    "        ind_names.append('Run ' + str(i))#print(rmse_err)\n",
    "    errors = pd.DataFrame(list(zip(train_avg_err,test_avg_err,train_rmse_err,test_rmse_err)),columns=['Train AAE','Test AAE','Train RMSE','Test RMSE'],index=ind_names).round(decimals=dec_points)\n",
    "\n",
    "    mean_list=[]\n",
    "    ci_list=[]\n",
    "\n",
    "    for column in errors:\n",
    "        mean_list.append(np.mean(errors[column]))\n",
    "        ci_list.append(stt.t.interval(ci_int_val,len(errors[column])-1, np.mean(errors[column]), stt.sem(errors[column]))) \n",
    "    errors.loc['Mean'] = re_round(mean_list,dec_points)\n",
    "    errors.loc['95% CI'] = re_round(ci_list,dec_points)\n",
    "\n",
    "    calc_parameters = pd.DataFrame(np.around(parameters_from_runs,dec_points),index=ind_names)\n",
    "    mean_list_p = []\n",
    "    ci_list_p = []\n",
    "    for column in calc_parameters:\n",
    "        mean_list_p.append(np.mean(calc_parameters[column]))\n",
    "        ci_list_p.append(stt.t.interval(ci_int_val,len(calc_parameters[column])-1, np.mean(calc_parameters[column]), stt.sem(calc_parameters[column])))\n",
    "    calc_parameters.loc['Mean'] = re_round(mean_list_p,dec_points)\n",
    "    calc_parameters.loc['95% CI'] = re_round(ci_list_p,dec_points)\n",
    "    model_dict = {\n",
    "        'Plot': fig,\n",
    "        'Errors': errors,\n",
    "        'Parameters': calc_parameters\n",
    "    }\n",
    "    plt.close()\n",
    "    return model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def ml_model_vbt_features_multipleruns(vbt_dataset_dict,dataset_key,alpha=1,number_of_runs=5):\n",
    "    \n",
    "    full_dataset = vbt_dataset_dict[dataset_key][['sigma','tau','V_m (nm3)','T_m (K)','Eccentricity(Ear)','Eccentricity(Eal)']]\n",
    "\n",
    "    train_avg_err=[]\n",
    "    test_avg_err=[]\n",
    "    train_rmse_err=[]\n",
    "    test_rmse_err=[]\n",
    "    runs = [\"Run \" + str(i+1) for i in range(number_of_runs)]\n",
    "    weights = pd.DataFrame(columns = ['sigma','tau','V_m (nm3)','Eccentricity(Ear)','Eccentricity(Eal)'],index=runs)\n",
    "\n",
    "    count = 1\n",
    "    while count < number_of_runs + 1:\n",
    "        [trainset,testset] = split_data(full_dataset)\n",
    "        y = (trainset['T_m (K)']-273).tolist()\n",
    "        y_test = (testset['T_m (K)']-273).tolist()\n",
    "        trainset = trainset.drop(columns = ['T_m (K)'])\n",
    "        testset = testset.drop(columns = ['T_m (K)'])\n",
    "        X = preprocessing.scale(trainset)\n",
    "        scaler = preprocessing.StandardScaler().fit(trainset)\n",
    "        X_test = scaler.transform(testset)\n",
    "        rr = Ridge(alpha=alpha)\n",
    "        rr.fit(X, y)\n",
    "        w = rr.coef_\n",
    "        intercept = rr.intercept_\n",
    "        \n",
    "        train_avg_err.append(np.mean(np.abs(y-(np.dot(X,w)+intercept))))\n",
    "        test_avg_err.append(np.mean(np.abs(y_test-(np.dot(X_test,w)+intercept))))\n",
    "        train_rmse_err.append(np.sqrt(((y - (np.dot(X,w)+intercept)) ** 2).mean()))\n",
    "        test_rmse_err.append(np.sqrt(((y_test-(np.dot(X_test,w)+intercept))**2).mean()))\n",
    "        \n",
    "        weights.loc[\"Run \" + str(count),:] = w\n",
    "        count = count + 1\n",
    "    \n",
    "    ci_int_val=0.95\n",
    "    dec_points=3\n",
    "\n",
    "    errors = pd.DataFrame(list(zip(train_avg_err,test_avg_err,train_rmse_err,test_rmse_err)),columns=['Train AAE','Test AAE','Train RMSE','Test RMSE'],index=runs).round(decimals=dec_points)\n",
    "\n",
    "    mean_list = [np.mean(errors[column]) for column in errors]\n",
    "    ci_list = [stt.t.interval(ci_int_val,len(errors[column])-1, np.mean(errors[column]), stt.sem(errors[column])) for column in errors]\n",
    "    errors.loc['Mean'] = np.around(mean_list,dec_points)\n",
    "    errors.loc['95% CI'] = re_round(ci_list,dec_points)\n",
    "\n",
    "    mean_list = [np.mean(weights[column]) for column in weights]\n",
    "    ci_list = [stt.t.interval(ci_int_val,len(weights[column])-1, np.mean(weights[column]), stt.sem(weights[column])) for column in weights]\n",
    "    weights.loc['Mean'] = np.around(mean_list,dec_points)\n",
    "    weights.loc['95% CI'] = re_round(ci_list,dec_points)\n",
    "\n",
    "    # Make plot\n",
    "    model_plot=plt.figure(figsize=(4,4), dpi=300)\n",
    "    ax1 = plt.gca()\n",
    "    ml_plot_train_points=ax1.scatter(y,np.dot(X,w)+intercept,c='k',alpha=1,linewidth=0)\n",
    "    ax1.plot([-273,2000],[-273,2000],'k--',lw=1)\n",
    "    ml_plot_test_points=ax1.scatter(y_test,np.dot(X_test,w)+intercept,c='r',alpha=1,linewidth=0)\n",
    "    lims = [min(y+y_test)-15,max(y+y_test)+15]\n",
    "    for theaxis in [ax1]:        \n",
    "        theaxis.set_aspect(1)\n",
    "        theaxis.set_xlim(lims)\n",
    "        theaxis.set_ylim(lims)\n",
    "        theaxis.set_xlabel(r\"Experimental $T_m$ ($^{\\circ}$C)\")\n",
    "        theaxis.set_ylabel(r\"Predicted $T_m$ ($^{\\circ}$C)\")\n",
    "        \n",
    "        for item in ([theaxis.xaxis.label, theaxis.yaxis.label, theaxis.yaxis.get_offset_text(), theaxis.xaxis.get_offset_text()]):\n",
    "            item.set_fontsize(12)\n",
    "        for item in (theaxis.get_xticklabels() + theaxis.get_yticklabels()):\n",
    "            item.set_fontsize(10)\n",
    "    plt.legend((ml_plot_train_points,ml_plot_test_points),('Training Set','Test Set'))    \n",
    "    plt.gcf().subplots_adjust(left=0.2,top=0.95,bottom=0.15,right=0.95)\n",
    "   \n",
    "    plt.close()\n",
    "    \n",
    "    ml_model_dict={\n",
    "        'Plot': model_plot,\n",
    "        'Errors': errors,\n",
    "        'Model Coefficients': weights\n",
    "    }\n",
    "    return ml_model_dict\n",
    "\n",
    "def rf_model_vbt_features_multipleruns(vbt_dataset_dict,dataset_key,number_of_runs=5):\n",
    "    \n",
    "    full_dataset = vbt_dataset_dict[dataset_key][['sigma','tau','V_m (nm3)','T_m (K)','Eccentricity(Ear)','Eccentricity(Eal)']]\n",
    "\n",
    "    train_avg_err=[]\n",
    "    test_avg_err=[]\n",
    "    train_rmse_err=[]\n",
    "    test_rmse_err=[]\n",
    "    runs = [\"Run \" + str(i+1) for i in range(number_of_runs)]\n",
    "    weights = pd.DataFrame(columns = ['sigma','tau','V_m (nm3)','Eccentricity(Ear)','Eccentricity(Eal)'],index=runs)\n",
    "\n",
    "    count = 1\n",
    "    while count < number_of_runs + 1:\n",
    "        [trainset,testset] = split_data(full_dataset)\n",
    "        y = (trainset['T_m (K)']-273).tolist()\n",
    "        y_test = (testset['T_m (K)']-273).tolist()\n",
    "        trainset = trainset.drop(columns = ['T_m (K)'])\n",
    "        testset = testset.drop(columns = ['T_m (K)'])\n",
    "        X = preprocessing.scale(trainset)\n",
    "        scaler = preprocessing.StandardScaler().fit(trainset)\n",
    "        X_test = scaler.transform(testset)\n",
    "        rf = RandomForestRegressor()\n",
    "        rf.fit(X, y)\n",
    "        # w = rr.coef_\n",
    "        # intercept = rr.intercept_\n",
    "        \n",
    "        train_avg_err.append(np.mean(np.abs(y-(rf.predict(X)))))\n",
    "        test_avg_err.append(np.mean(np.abs(y_test-(rf.predict(X_test)))))\n",
    "        train_rmse_err.append(np.sqrt(((y - (rf.predict(X))) ** 2).mean()))\n",
    "        test_rmse_err.append(np.sqrt(((y_test-(rf.predict(X_test)))**2).mean()))\n",
    "        \n",
    "        weights.loc[\"Run \" + str(count),:] = rf.feature_importances_\n",
    "        count = count + 1\n",
    "    \n",
    "    ci_int_val=0.95\n",
    "    dec_points=3\n",
    "\n",
    "    errors = pd.DataFrame(list(zip(train_avg_err,test_avg_err,train_rmse_err,test_rmse_err)),columns=['Train AAE','Test AAE','Train RMSE','Test RMSE'],index=runs).round(decimals=dec_points)\n",
    "\n",
    "    mean_list = [np.mean(errors[column]) for column in errors]\n",
    "    ci_list = [stt.t.interval(ci_int_val,len(errors[column])-1, np.mean(errors[column]), stt.sem(errors[column])) for column in errors]\n",
    "    errors.loc['Mean'] = np.around(mean_list,dec_points)\n",
    "    errors.loc['95% CI'] = re_round(ci_list,dec_points)\n",
    "\n",
    "    mean_list = [np.mean(weights[column]) for column in weights]\n",
    "    ci_list = [stt.t.interval(ci_int_val,len(weights[column])-1, np.mean(weights[column]), stt.sem(weights[column])) for column in weights]\n",
    "    weights.loc['Mean'] = np.around(mean_list,dec_points)\n",
    "    weights.loc['95% CI'] = re_round(ci_list,dec_points)\n",
    "\n",
    "    # Make plot\n",
    "    model_plot=plt.figure(figsize=(4,4), dpi=300)\n",
    "    ax1 = plt.gca()\n",
    "    ml_plot_train_points=ax1.scatter(y,rf.predict(X),c='k',alpha=1,linewidth=0)\n",
    "    ax1.plot([-273,2000],[-273,2000],'k--',lw=1)\n",
    "    ml_plot_test_points=ax1.scatter(y_test,rf.predict(X_test),c='r',alpha=1,linewidth=0)\n",
    "    lims = [min(y+y_test)-15,max(y+y_test)+15]\n",
    "    for theaxis in [ax1]:        \n",
    "        theaxis.set_aspect(1)\n",
    "        theaxis.set_xlim(lims)\n",
    "        theaxis.set_ylim(lims)\n",
    "        theaxis.set_xlabel(r\"Experimental $T_m$ ($^{\\circ}$C)\")\n",
    "        theaxis.set_ylabel(r\"Predicted $T_m$ ($^{\\circ}$C)\")\n",
    "        \n",
    "        for item in ([theaxis.xaxis.label, theaxis.yaxis.label, theaxis.yaxis.get_offset_text(), theaxis.xaxis.get_offset_text()]):\n",
    "            item.set_fontsize(12)\n",
    "        for item in (theaxis.get_xticklabels() + theaxis.get_yticklabels()):\n",
    "            item.set_fontsize(10)\n",
    "    plt.legend((ml_plot_train_points,ml_plot_test_points),('Training Set','Test Set'))    \n",
    "    plt.gcf().subplots_adjust(left=0.2,top=0.95,bottom=0.15,right=0.95)\n",
    "   \n",
    "    plt.close()\n",
    "    \n",
    "    ml_model_dict={\n",
    "        'Plot': model_plot,\n",
    "        'Errors': errors,\n",
    "        'Feature Importances': weights\n",
    "    }\n",
    "    return ml_model_dict\n",
    "\n",
    "def ml_model_multipleruns(ml_dataset_dict, dataset_key, alpha=100, number_of_runs=5, do_featurization = False):  \n",
    "    if do_featurization:\n",
    "\n",
    "        working_ML_dataset=ml_dataset_dict[dataset_key]\n",
    "        mols_full=[Chem.MolFromSmiles(m) for m in working_ML_dataset.SMILES.tolist() if Chem.MolFromSmiles(m) != None]\n",
    "        calc = Calculator(descriptors, ignore_3D=True)\n",
    "        mordredresults = calc.pandas(mols_full)\n",
    "        working_ML_dataset = working_ML_dataset.join(mordredresults,how='inner')\n",
    "        # save to the appropriate file name depending on which dataset you're looking at\n",
    "        if dataset_key=='Quinones':\n",
    "            working_ML_dataset.to_csv('Data Files/featurized_bq.csv',index=False)\n",
    "        elif dataset_key=='Hydroquinones':\n",
    "            working_ML_dataset.to_csv('Data Files/featurized_hq.csv',index=False)\n",
    "        else:\n",
    "            print('Not a valid dataset key')            \n",
    "    else:\n",
    "        if dataset_key=='Quinones':\n",
    "            working_ML_dataset=pd.read_csv('Data Files/featurized_bq.csv',low_memory=False)\n",
    "        elif dataset_key=='Hydroquinones':\n",
    "            working_ML_dataset=pd.read_csv('Data Files/featurized_hq.csv',low_memory=False)\n",
    "\n",
    "    train_avg_err=[]\n",
    "    test_avg_err=[]\n",
    "    train_rmse_err=[]\n",
    "    test_rmse_err=[]\n",
    "    runs = [\"Run \" + str(i+1) for i in range(number_of_runs)]\n",
    "    weights = pd.DataFrame(columns = working_ML_dataset.columns.tolist(),index=runs)\n",
    "\n",
    "    count = 1\n",
    "    while count < number_of_runs + 1:\n",
    "        [trainset,testset] = split_data(working_ML_dataset)\n",
    "        trainset = trainset.reset_index()\n",
    "        testset = testset.reset_index()        \n",
    "        \n",
    "        ######\n",
    "        ## Standardization (want each feature to be a gaussian with zero mean and unit variance)\n",
    "        ######\n",
    "\n",
    "        # drop non-numeric columns and ones for the melting point, so we only have columns of features\n",
    "        # I don't know why, but LogP caused a problem during the standardization - dropping for now, but have to figure out\n",
    "        # Now dropping everything but MW from the Reaxys, since we don't have it for the Na and K salts\n",
    "        # columns_to_drop_from_reaxys = ['InChI Key','SMILES','Type of Substance','mp_mean','mp_std','LogP','H Bond Donors','H Bond Acceptors','Rotatable Bonds','TPSA','Lipinski Number','Veber Number','MAXdO','MINdO']\n",
    "        columns_to_drop = ['InChI Key','SMILES','Type of Substance','mp_mean','mp_std','LogP','H Bond Donors','H Bond Acceptors','Rotatable Bonds','TPSA','Lipinski Number','Veber Number','MAXdO','MINdO','index']\n",
    "\n",
    "        # if dataset_key=='Quinones':\n",
    "        #     columns_to_drop_thatgavetrouble = ['MAXdO','MINdO']\n",
    "        #     # for bq these gave trouble: ['Unnamed: 0','MAXdO','MINdO']\n",
    "        # elif dataset_key=='Hydroquinones':\n",
    "        #     columns_to_drop_thatgavetrouble = ['MAXdO','MINdO']\n",
    "\n",
    "        # columns_to_drop = columns_to_drop_from_reaxys + columns_to_drop_thatgavetrouble\n",
    "        trainset_s = trainset.drop(columns=columns_to_drop)\n",
    "        testset_s = testset.drop(columns=columns_to_drop)\n",
    "\n",
    "        # drop columns where there is an error from mordred\n",
    "        #print('Started with '+str(len(trainset_s.columns))+' features')\n",
    "        trainset_s = trainset_s.select_dtypes(include=['float64','int'])\n",
    "\n",
    "        #print('After dropping columns with mordred errors, have '+str(len(trainset_s.columns))+' features')\n",
    "\n",
    "        # drop the same columns from the test set\n",
    "        testset_s = testset_s[trainset_s.columns]\n",
    "\n",
    "        # finally, do the standardization\n",
    "        X = preprocessing.scale(trainset_s)\n",
    "        # apply the same standardization to the test set\n",
    "        scaler = preprocessing.StandardScaler().fit(trainset_s)\n",
    "        X_test = scaler.transform(testset_s)\n",
    "\n",
    "        ######\n",
    "        ## Ridge regression\n",
    "        ######\n",
    "\n",
    "        y = trainset.mp_mean.tolist()\n",
    "        y_test = testset.mp_mean.tolist()\n",
    "\n",
    "        rr = Ridge(alpha=alpha)\n",
    "        rr.fit(X, y)\n",
    "        w = rr.coef_\n",
    "        intercept = rr.intercept_\n",
    "\n",
    "        # avg_abs_err = np.zeros(2)\n",
    "        # rmse_err = np.zeros(2)\n",
    "        # The test set is at the 0 index and the training set is at the 1 index to match the convention in the other model \n",
    "        train_avg_err.append(np.mean(np.abs(y-(np.dot(X,w)+intercept))))\n",
    "        test_avg_err.append(np.mean(np.abs(y_test-(np.dot(X_test,w)+intercept))))\n",
    "        train_rmse_err.append(np.sqrt(((y - (np.dot(X,w)+intercept)) ** 2).mean()))\n",
    "        test_rmse_err.append(np.sqrt(((y_test-(np.dot(X_test,w)+intercept))**2).mean()))\n",
    "\n",
    "        if count == 1:\n",
    "            weights = pd.DataFrame(index=runs,columns=trainset_s.columns)\n",
    "        # coeffs = pd.DataFrame(data={'label':trainset_s.columns, 'w':w, 'w_abs':np.abs(w)})\n",
    "        weights.loc[\"Run \" + str(count),:] = w\n",
    "\n",
    "        count = count + 1\n",
    "    \n",
    "    ci_int_val=0.95\n",
    "    dec_points=3\n",
    "    \n",
    "    errors = pd.DataFrame(list(zip(train_avg_err,test_avg_err,train_rmse_err,test_rmse_err)),columns=['Train AAE','Test AAE','Train RMSE','Test RMSE'],index=runs).round(decimals=dec_points)\n",
    "\n",
    "    mean_list = [np.mean(errors[column]) for column in errors]\n",
    "    ci_list = [stt.t.interval(ci_int_val,len(errors[column])-1, np.mean(errors[column]), stt.sem(errors[column])) for column in errors]\n",
    "    errors.loc['Mean'] = np.around(mean_list,dec_points)\n",
    "    errors.loc['95% CI'] = re_round(ci_list,dec_points)\n",
    "\n",
    "    mean_list = [np.mean(weights[column]) for column in weights]\n",
    "    ci_list = [stt.t.interval(ci_int_val,len(weights[column])-1, np.mean(weights[column]), stt.sem(weights[column])) for column in weights]\n",
    "    weights.loc['Mean'] = np.around(mean_list,dec_points)\n",
    "    weights.loc[\"Absolute Mean\"] = np.abs(weights.loc[\"Mean\"])\n",
    "    weights.loc['95% CI'] = re_round(ci_list,dec_points)\n",
    "    # Sort by the mean weight in descending order\n",
    "    weights.sort_values(by=[\"Absolute Mean\"],axis=1,ascending=False,inplace=True)\n",
    "\n",
    "    # Make plot\n",
    "    model_plot=plt.figure(figsize=(4,4), dpi=300)\n",
    "    ax1 = plt.gca()\n",
    "    ml_plot_train_points=ax1.scatter(y,np.dot(X,w)+intercept,s=5,c='k',alpha=0.7,linewidth=0)\n",
    "    ax1.plot([-273,2000],[-273,2000],'k--',lw=1)\n",
    "    ml_plot_test_points=ax1.scatter(y_test,np.dot(X_test,w)+intercept,s=5,c='r',alpha=0.7,linewidth=0)\n",
    "\n",
    "    lims = [min(y+y_test)-5,max(y+y_test)+5]\n",
    "\n",
    "    for theaxis in [ax1]:\n",
    "        \n",
    "        theaxis.set_aspect(1)\n",
    "        theaxis.set_xlim(lims)\n",
    "        theaxis.set_ylim(lims)\n",
    "\n",
    "        theaxis.set_xlabel(r\"Experimental $T_m$ ($^{\\circ}$C)\")\n",
    "        theaxis.set_ylabel(r\"Predicted $T_m$ ($^{\\circ}$C)\")\n",
    "        \n",
    "        for item in ([theaxis.xaxis.label, theaxis.yaxis.label, theaxis.yaxis.get_offset_text(), theaxis.xaxis.get_offset_text()]):\n",
    "            item.set_fontsize(12)\n",
    "        for item in (theaxis.get_xticklabels() + theaxis.get_yticklabels()):\n",
    "            item.set_fontsize(10)\n",
    "    plt.legend((ml_plot_train_points,ml_plot_test_points),('Training Set','Test Set'))    \n",
    "    plt.gcf().subplots_adjust(left=0.2,top=0.95,bottom=0.15,right=0.95)\n",
    "   \n",
    "    plt.close()\n",
    "\n",
    "    ml_model_dict={\n",
    "        'Plot': model_plot,\n",
    "        'Errors': errors,\n",
    "        'Model Coefficients': weights\n",
    "    }\n",
    "    return ml_model_dict\n",
    "\n",
    "def rf_model_multipleruns(ml_dataset_dict, dataset_key, number_of_runs=5, do_featurization = False):  \n",
    "    if do_featurization:\n",
    "\n",
    "        working_ML_dataset=ml_dataset_dict[dataset_key]\n",
    "        mols_full=[Chem.MolFromSmiles(m) for m in working_ML_dataset.SMILES.tolist() if Chem.MolFromSmiles(m) != None]\n",
    "        calc = Calculator(descriptors, ignore_3D=True)\n",
    "        mordredresults = calc.pandas(mols_full)\n",
    "        working_ML_dataset = working_ML_dataset.join(mordredresults,how='inner')\n",
    "        # save to the appropriate file name depending on which dataset you're looking at\n",
    "        if dataset_key=='Quinones':\n",
    "            working_ML_dataset.to_csv('Data Files/featurized_bq.csv',index=False)\n",
    "        elif dataset_key=='Hydroquinones':\n",
    "            working_ML_dataset.to_csv('Data Files/featurized_hq.csv',index=False)\n",
    "        else:\n",
    "            print('Not a valid dataset key')            \n",
    "    else:\n",
    "        if dataset_key=='Quinones':\n",
    "            working_ML_dataset=pd.read_csv('Data Files/featurized_bq.csv',low_memory=False)\n",
    "        elif dataset_key=='Hydroquinones':\n",
    "            working_ML_dataset=pd.read_csv('Data Files/featurized_hq.csv',low_memory=False)\n",
    "\n",
    "    train_avg_err=[]\n",
    "    test_avg_err=[]\n",
    "    train_rmse_err=[]\n",
    "    test_rmse_err=[]\n",
    "    runs = [\"Run \" + str(i+1) for i in range(number_of_runs)]\n",
    "    weights = pd.DataFrame(columns = working_ML_dataset.columns.tolist(),index=runs)\n",
    "\n",
    "    count = 1\n",
    "    while count < number_of_runs + 1:\n",
    "        [trainset,testset] = split_data(working_ML_dataset)\n",
    "        trainset = trainset.reset_index()\n",
    "        testset = testset.reset_index()        \n",
    "        \n",
    "        ######\n",
    "        ## Standardization (want each feature to be a gaussian with zero mean and unit variance)\n",
    "        ######\n",
    "\n",
    "        columns_to_drop = ['InChI Key','SMILES','Type of Substance','mp_mean','mp_std','LogP','H Bond Donors','H Bond Acceptors','Rotatable Bonds','TPSA','Lipinski Number','Veber Number','MAXdO','MINdO','index']\n",
    "\n",
    "        trainset_s = trainset.drop(columns=columns_to_drop)\n",
    "        testset_s = testset.drop(columns=columns_to_drop)\n",
    "\n",
    "        # drop columns where there is an error from mordred\n",
    "        trainset_s = trainset_s.select_dtypes(include=['float64','int'])\n",
    "\n",
    "        # drop the same columns from the test set\n",
    "        testset_s = testset_s[trainset_s.columns]\n",
    "\n",
    "        # finally, do the standardization\n",
    "        X = preprocessing.scale(trainset_s)\n",
    "        # apply the same standardization to the test set\n",
    "        scaler = preprocessing.StandardScaler().fit(trainset_s)\n",
    "        X_test = scaler.transform(testset_s)\n",
    "\n",
    "        ######\n",
    "        ## Random Forest regression\n",
    "        ######\n",
    "\n",
    "        y = trainset.mp_mean.tolist()\n",
    "        y_test = testset.mp_mean.tolist()\n",
    "\n",
    "        rf = RandomForestRegressor(n_jobs=-1) # In theory, this speeds up the process\n",
    "        rf.fit(X, y)\n",
    "        # w = rr.coef_\n",
    "        # intercept = rr.intercept_\n",
    "\n",
    "        train_avg_err.append(np.mean(np.abs(y-rf.predict(X))))\n",
    "        test_avg_err.append(np.mean(np.abs(y_test-rf.predict(X_test))))\n",
    "        train_rmse_err.append(np.sqrt(((y - rf.predict(X)) ** 2).mean()))\n",
    "        test_rmse_err.append(np.sqrt(((y_test- rf.predict(X_test))**2).mean()))\n",
    "\n",
    "        if count == 1:\n",
    "            weights = pd.DataFrame(index=runs,columns=trainset_s.columns)\n",
    "        weights.loc[\"Run \" + str(count),:] = rf.feature_importances_\n",
    "\n",
    "        count = count + 1\n",
    "    \n",
    "    ci_int_val=0.95\n",
    "    dec_points=3\n",
    "    \n",
    "    errors = pd.DataFrame(list(zip(train_avg_err,test_avg_err,train_rmse_err,test_rmse_err)),columns=['Train AAE','Test AAE','Train RMSE','Test RMSE'],index=runs).round(decimals=dec_points)\n",
    "\n",
    "    mean_list = [np.mean(errors[column]) for column in errors]\n",
    "    ci_list = [stt.t.interval(ci_int_val,len(errors[column])-1, np.mean(errors[column]), stt.sem(errors[column])) for column in errors]\n",
    "    errors.loc['Mean'] = np.around(mean_list,dec_points)\n",
    "    errors.loc['95% CI'] = re_round(ci_list,dec_points)\n",
    "\n",
    "    mean_list = [np.mean(weights[column]) for column in weights]\n",
    "    ci_list = [stt.t.interval(ci_int_val,len(weights[column])-1, np.mean(weights[column]), stt.sem(weights[column])) for column in weights]\n",
    "    weights.loc['Mean'] = np.around(mean_list,dec_points)\n",
    "    weights.loc[\"Absolute Mean\"] = np.abs(weights.loc[\"Mean\"])\n",
    "    weights.loc['95% CI'] = re_round(ci_list,dec_points)\n",
    "    # Sort by the mean weight in descending order\n",
    "    weights.sort_values(by=[\"Absolute Mean\"],axis=1,ascending=False,inplace=True)\n",
    "\n",
    "    # Make plot\n",
    "    model_plot=plt.figure(figsize=(4,4), dpi=300)\n",
    "    ax1 = plt.gca()\n",
    "    ml_plot_train_points=ax1.scatter(y,rf.predict(X),s=5,c='k',alpha=0.7,linewidth=0)\n",
    "    ax1.plot([-273,2000],[-273,2000],'k--',lw=1)\n",
    "    ml_plot_test_points=ax1.scatter(y_test,rf.predict(X_test),s=5,c='r',alpha=0.7,linewidth=0)\n",
    "\n",
    "    lims = [min(y+y_test)-5,max(y+y_test)+5]\n",
    "\n",
    "    for theaxis in [ax1]:\n",
    "        \n",
    "        theaxis.set_aspect(1)\n",
    "        theaxis.set_xlim(lims)\n",
    "        theaxis.set_ylim(lims)\n",
    "\n",
    "        theaxis.set_xlabel(r\"Experimental $T_m$ ($^{\\circ}$C)\")\n",
    "        theaxis.set_ylabel(r\"Predicted $T_m$ ($^{\\circ}$C)\")\n",
    "        \n",
    "        for item in ([theaxis.xaxis.label, theaxis.yaxis.label, theaxis.yaxis.get_offset_text(), theaxis.xaxis.get_offset_text()]):\n",
    "            item.set_fontsize(12)\n",
    "        for item in (theaxis.get_xticklabels() + theaxis.get_yticklabels()):\n",
    "            item.set_fontsize(10)\n",
    "    plt.legend((ml_plot_train_points,ml_plot_test_points),('Training Set','Test Set'))    \n",
    "    plt.gcf().subplots_adjust(left=0.2,top=0.95,bottom=0.15,right=0.95)\n",
    "   \n",
    "    plt.close()\n",
    "\n",
    "    ml_model_dict={\n",
    "        'Plot': model_plot,\n",
    "        'Errors': errors,\n",
    "        'Feature Importances': weights\n",
    "    }\n",
    "    return ml_model_dict\n",
    "\n",
    "def ml_model_vbt_features_tau_inputs_multipleruns(vbt_dataset_dict,dataset_key,alpha=1,number_of_runs=5):\n",
    "    \n",
    "    full_dataset = vbt_dataset_dict[dataset_key][['sigma','SP3 (LIN)','SP3 (BR)','SP2','Ring','V_m (nm3)','T_m (K)','Eccentricity(Ear)','Eccentricity(Eal)']]\n",
    "\n",
    "    train_avg_err=[]\n",
    "    test_avg_err=[]\n",
    "    train_rmse_err=[]\n",
    "    test_rmse_err=[]\n",
    "    runs = [\"Run \" + str(i+1) for i in range(number_of_runs)]\n",
    "    weights = pd.DataFrame(columns = ['sigma','SP3 (LIN)','SP3 (BR)','SP2','Ring','V_m (nm3)','Eccentricity(Ear)','Eccentricity(Eal)'],index=runs)\n",
    "\n",
    "    count = 1\n",
    "    while count < number_of_runs + 1:\n",
    "        [trainset,testset] = split_data(full_dataset)\n",
    "        y = (trainset['T_m (K)']-273).tolist()\n",
    "        y_test = (testset['T_m (K)']-273).tolist()\n",
    "        trainset = trainset.drop(columns = ['T_m (K)'])\n",
    "        testset = testset.drop(columns = ['T_m (K)'])\n",
    "        X = preprocessing.scale(trainset)\n",
    "        scaler = preprocessing.StandardScaler().fit(trainset)\n",
    "        X_test = scaler.transform(testset)\n",
    "        rr = Ridge(alpha=alpha)\n",
    "        rr.fit(X, y)\n",
    "        w = rr.coef_\n",
    "        intercept = rr.intercept_\n",
    "        \n",
    "        train_avg_err.append(np.mean(np.abs(y-(np.dot(X,w)+intercept))))\n",
    "        test_avg_err.append(np.mean(np.abs(y_test-(np.dot(X_test,w)+intercept))))\n",
    "        train_rmse_err.append(np.sqrt(((y - (np.dot(X,w)+intercept)) ** 2).mean()))\n",
    "        test_rmse_err.append(np.sqrt(((y_test-(np.dot(X_test,w)+intercept))**2).mean()))\n",
    "        \n",
    "        weights.loc[\"Run \" + str(count),:] = w\n",
    "        count = count + 1\n",
    "    \n",
    "    ci_int_val=0.95\n",
    "    dec_points=3\n",
    "\n",
    "    errors = pd.DataFrame(list(zip(train_avg_err,test_avg_err,train_rmse_err,test_rmse_err)),columns=['Train AAE','Test AAE','Train RMSE','Test RMSE'],index=runs).round(decimals=dec_points)\n",
    "\n",
    "    mean_list = [np.mean(errors[column]) for column in errors]\n",
    "    ci_list = [stt.t.interval(ci_int_val,len(errors[column])-1, np.mean(errors[column]), stt.sem(errors[column])) for column in errors]\n",
    "    errors.loc['Mean'] = np.around(mean_list,dec_points)\n",
    "    errors.loc['95% CI'] = re_round(ci_list,dec_points)\n",
    "\n",
    "    mean_list = [np.mean(weights[column]) for column in weights]\n",
    "    ci_list = [stt.t.interval(ci_int_val,len(weights[column])-1, np.mean(weights[column]), stt.sem(weights[column])) for column in weights]\n",
    "    weights.loc['Mean'] = np.around(mean_list,dec_points)\n",
    "    weights.loc['95% CI'] = re_round(ci_list,dec_points)\n",
    "\n",
    "    # Make plot\n",
    "    model_plot=plt.figure(figsize=(4,4), dpi=300)\n",
    "    ax1 = plt.gca()\n",
    "    ml_plot_train_points=ax1.scatter(y,np.dot(X,w)+intercept,c='k',alpha=1,linewidth=0)\n",
    "    ax1.plot([-273,2000],[-273,2000],'k--',lw=1)\n",
    "    ml_plot_test_points=ax1.scatter(y_test,np.dot(X_test,w)+intercept,c='r',alpha=1,linewidth=0)\n",
    "    lims = [min(y+y_test)-15,max(y+y_test)+15]\n",
    "    for theaxis in [ax1]:        \n",
    "        theaxis.set_aspect(1)\n",
    "        theaxis.set_xlim(lims)\n",
    "        theaxis.set_ylim(lims)\n",
    "        theaxis.set_xlabel(r\"Experimental $T_m$ ($^{\\circ}$C)\")\n",
    "        theaxis.set_ylabel(r\"Predicted $T_m$ ($^{\\circ}$C)\")\n",
    "        \n",
    "        for item in ([theaxis.xaxis.label, theaxis.yaxis.label, theaxis.yaxis.get_offset_text(), theaxis.xaxis.get_offset_text()]):\n",
    "            item.set_fontsize(12)\n",
    "        for item in (theaxis.get_xticklabels() + theaxis.get_yticklabels()):\n",
    "            item.set_fontsize(10)\n",
    "    plt.legend((ml_plot_train_points,ml_plot_test_points),('Training Set','Test Set'))    \n",
    "    plt.gcf().subplots_adjust(left=0.2,top=0.95,bottom=0.15,right=0.95)\n",
    "   \n",
    "    plt.close()\n",
    "    \n",
    "    ml_model_dict={\n",
    "        'Plot': model_plot,\n",
    "        'Errors': errors,\n",
    "        'Model Coefficients': weights\n",
    "    }\n",
    "    return ml_model_dict\n",
    "\n",
    "def rf_model_vbt_features_tau_inputs_multipleruns(vbt_dataset_dict,dataset_key,number_of_runs=5):\n",
    "    \n",
    "    full_dataset = vbt_dataset_dict[dataset_key][['sigma','SP3 (LIN)','SP3 (BR)','SP2','Ring','V_m (nm3)','T_m (K)','Eccentricity(Ear)','Eccentricity(Eal)']]\n",
    "\n",
    "    train_avg_err=[]\n",
    "    test_avg_err=[]\n",
    "    train_rmse_err=[]\n",
    "    test_rmse_err=[]\n",
    "    runs = [\"Run \" + str(i+1) for i in range(number_of_runs)]\n",
    "    weights = pd.DataFrame(columns = ['sigma','SP3 (LIN)','SP3 (BR)','SP2','Ring','V_m (nm3)','Eccentricity(Ear)','Eccentricity(Eal)'],index=runs)\n",
    "\n",
    "    count = 1\n",
    "    while count < number_of_runs + 1:\n",
    "        [trainset,testset] = split_data(full_dataset)\n",
    "        y = (trainset['T_m (K)']-273).tolist()\n",
    "        y_test = (testset['T_m (K)']-273).tolist()\n",
    "        trainset = trainset.drop(columns = ['T_m (K)'])\n",
    "        testset = testset.drop(columns = ['T_m (K)'])\n",
    "        X = preprocessing.scale(trainset)\n",
    "        scaler = preprocessing.StandardScaler().fit(trainset)\n",
    "        X_test = scaler.transform(testset)\n",
    "        rf = RandomForestRegressor()\n",
    "        rf.fit(X, y)\n",
    "        # w = rr.coef_\n",
    "        # intercept = rr.intercept_\n",
    "        \n",
    "        train_avg_err.append(np.mean(np.abs(y-(rf.predict(X)))))\n",
    "        test_avg_err.append(np.mean(np.abs(y_test-(rf.predict(X_test)))))\n",
    "        train_rmse_err.append(np.sqrt(((y - (rf.predict(X))) ** 2).mean()))\n",
    "        test_rmse_err.append(np.sqrt(((y_test-(rf.predict(X_test)))**2).mean()))\n",
    "        \n",
    "        weights.loc[\"Run \" + str(count),:] = rf.feature_importances_\n",
    "        count = count + 1\n",
    "    \n",
    "    ci_int_val=0.95\n",
    "    dec_points=3\n",
    "\n",
    "    errors = pd.DataFrame(list(zip(train_avg_err,test_avg_err,train_rmse_err,test_rmse_err)),columns=['Train AAE','Test AAE','Train RMSE','Test RMSE'],index=runs).round(decimals=dec_points)\n",
    "\n",
    "    mean_list = [np.mean(errors[column]) for column in errors]\n",
    "    ci_list = [stt.t.interval(ci_int_val,len(errors[column])-1, np.mean(errors[column]), stt.sem(errors[column])) for column in errors]\n",
    "    errors.loc['Mean'] = np.around(mean_list,dec_points)\n",
    "    errors.loc['95% CI'] = re_round(ci_list,dec_points)\n",
    "\n",
    "    mean_list = [np.mean(weights[column]) for column in weights]\n",
    "    ci_list = [stt.t.interval(ci_int_val,len(weights[column])-1, np.mean(weights[column]), stt.sem(weights[column])) for column in weights]\n",
    "    weights.loc['Mean'] = np.around(mean_list,dec_points)\n",
    "    weights.loc['95% CI'] = re_round(ci_list,dec_points)\n",
    "\n",
    "    # Make plot\n",
    "    model_plot=plt.figure(figsize=(4,4), dpi=300)\n",
    "    ax1 = plt.gca()\n",
    "    ml_plot_train_points=ax1.scatter(y,rf.predict(X),c='k',alpha=1,linewidth=0)\n",
    "    ax1.plot([-273,2000],[-273,2000],'k--',lw=1)\n",
    "    ml_plot_test_points=ax1.scatter(y_test,rf.predict(X_test),c='r',alpha=1,linewidth=0)\n",
    "    lims = [min(y+y_test)-15,max(y+y_test)+15]\n",
    "    for theaxis in [ax1]:        \n",
    "        theaxis.set_aspect(1)\n",
    "        theaxis.set_xlim(lims)\n",
    "        theaxis.set_ylim(lims)\n",
    "        theaxis.set_xlabel(r\"Experimental $T_m$ ($^{\\circ}$C)\")\n",
    "        theaxis.set_ylabel(r\"Predicted $T_m$ ($^{\\circ}$C)\")\n",
    "        \n",
    "        for item in ([theaxis.xaxis.label, theaxis.yaxis.label, theaxis.yaxis.get_offset_text(), theaxis.xaxis.get_offset_text()]):\n",
    "            item.set_fontsize(12)\n",
    "        for item in (theaxis.get_xticklabels() + theaxis.get_yticklabels()):\n",
    "            item.set_fontsize(10)\n",
    "    plt.legend((ml_plot_train_points,ml_plot_test_points),('Training Set','Test Set'))    \n",
    "    plt.gcf().subplots_adjust(left=0.2,top=0.95,bottom=0.15,right=0.95)\n",
    "   \n",
    "    plt.close()\n",
    "    \n",
    "    ml_model_dict={\n",
    "        'Plot': model_plot,\n",
    "        'Errors': errors,\n",
    "        'Feature Importances': weights\n",
    "    }\n",
    "    return ml_model_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quinone_data = pd.read_csv(\"Data Files/Entropy and Volume Data - Quinones.csv\")\n",
    "hydroquinone_data= pd.read_csv(\"Data Files/Entropy and Volume Data - Hydroquinones.csv\")\n",
    "hydrocarbon_data= pd.read_csv(\"Data Files/Entropy and Volume Data - Hydrocarbons.csv\")\n",
    "mega_database=pd.concat([hydroquinone_data,quinone_data])[['sigma','tau','V_m (nm3)','T_m (K)','Eccentricity(Ear)','Eccentricity(Eal)']].reset_index(drop=True)\n",
    "# ML Data\n",
    "quinone_ML_data=pd.read_csv('Data Files/parsed_p_benzoquinone_216.csv')\n",
    "hydroquinone_ML_data=pd.read_csv('Data Files/parsed_p_hydroquinone_204.csv')\n",
    "# Make dictionary for VBT data\n",
    "dataset_dict={'Quinones':quinone_data,'Hydroquinones':hydroquinone_data,'Hydrocarbons':hydrocarbon_data,'Quinones + Hydroquinones':mega_database}\n",
    "# Make dictionary for ML data\n",
    "ml_dataset_dict={'Quinones':quinone_ML_data.drop(columns='Unnamed: 0'),'Hydroquinones':hydroquinone_ML_data.drop(columns='Unnamed: 0')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dict = rf_model_vbt_features_tau_inputs_multipleruns(dataset_dict,\"Hydroquinones\",number_of_runs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dict['Plot']\n",
    "# ml_dict['Plot'].savefig('Plots/RandomForest_HQ_VBTFeatures_SepTau_plot.png',dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dict['Errors']\n",
    "# ml_dict[\"Errors\"].to_csv('Errors/RandomForest_HQ_VBTFeatures_SepTau_Errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dict[\"Feature Importances\"]\n",
    "# ml_dict[\"Feature Importances\"].to_csv('Parameters/RandomForest_HQ_VBTFeatures_SepTau_Parameters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermodynamic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_form= '(parameters[0]*predictors[\"V_m (nm3)\"]**(-1)+parameters[1])/(parameters[2]*np.log(predictors[\"sigma\"])+parameters[3]*predictors[\"tau\"]+1+parameters[4]*np.log(predictors[\"Eccentricity(Ear)\"])+parameters[5]*np.log(predictors[\"Eccentricity(Eal)\"]))'\n",
    "\n",
    "starting_guesses= [0,300,-0.01,0.01,-0.01,-0.01]\n",
    "dataset_name='Hydrocarbons'\n",
    "num_runs=100\n",
    "vbt_bq_dict=vbt_model_automated(dataset_dict,dataset_name,model_form,starting_guesses,num_runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbt_bq_dict['Parameters'].rename(columns={0:'g',1:'h',2:'a',3:'b',4:'c',5:'d'})\n",
    "# .to_csv(\"Parameters/Thermodynamic_HC_Parameters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbt_bq_dict[\"Errors\"]\n",
    "# .to_csv(\"Errors/Thermodynamic_HC_Errors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbt_bq_dict[\"Plot\"]\n",
    "# .savefig('Plots/Thermodynamic_HC_plot.png',dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('mppredictionpapervenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45f8f9919628e68015e8dea04f2358c2cb597463fcba8ded03455243189a3041"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
